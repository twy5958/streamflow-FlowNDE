2024-06-04 21:38:46.365 | INFO     | __main__:main:85 - {'remote': False, 'device': 0, 'epochs': 200, 'seed': 42, 'batch_size': 16, 'filename': 'PEMS04', 'train_ratio': 0.6, 'valid_ratio': 0.2, 'his_length': 12, 'pred_length': 12, 'num_layers': 1, 'lr': 0.001, 'step_size': 1, 'hidden_dim': 64, 'back': 3, 'thres': 0.005, 'log': 'TRUE'}
2024-06-04 21:43:53.204 | INFO     | __main__:main:138 - =====Epoch 1=====

##on train data## total loss: 0.0757830652097861, pred loss: 0.049768227820267096, regression loss: 0.026014837389519006 
##on train data## rmse loss: 39.57001530149448, mae loss: 26.20550256705134, mape loss: 21.306677626551323
##on valid data## rmse loss: 39.28229110500824, mae loss: 27.305882598551527, mape loss: 20.674232100423477
##on test data## rmse loss: 37.4597725077263, mae loss: 26.246726302739003, mape loss: 19.91336604500834

2024-06-04 21:48:34.734 | INFO     | __main__:main:138 - =====Epoch 2=====

##on train data## total loss: 0.03826475092180589, pred loss: 0.03039008208679375, regression loss: 0.007874668835012137 
##on train data## rmse loss: 39.99115980196299, mae loss: 27.378125175739985, mape loss: 21.808700263500214
##on valid data## rmse loss: 39.850986873934055, mae loss: 28.412635622431317, mape loss: 21.416276555631963
##on test data## rmse loss: 38.2272308901023, mae loss: 27.50077270670525, mape loss: 20.763161587771645

2024-06-04 21:53:08.900 | INFO     | __main__:main:138 - =====Epoch 3=====

##on train data## total loss: 0.03629264860771268, pred loss: 0.029246552983143676, regression loss: 0.007046095624569007 
##on train data## rmse loss: 37.77306409751844, mae loss: 24.713191653197665, mape loss: 19.677350128877837
##on valid data## rmse loss: 37.56091142717696, mae loss: 25.78355559805558, mape loss: 19.092009470784834
##on test data## rmse loss: 35.76654405729465, mae loss: 24.764673490659884, mape loss: 18.40618770057556

2024-06-04 21:57:43.424 | INFO     | __main__:main:138 - =====Epoch 4=====

##on train data## total loss: 0.03519589497865933, pred loss: 0.02841331425661795, regression loss: 0.006782580722041381 
##on train data## rmse loss: 37.46593436954906, mae loss: 25.180620232468133, mape loss: 24.195292046141326
##on valid data## rmse loss: 37.17009165614702, mae loss: 25.935323385265765, mape loss: 23.26816463074978
##on test data## rmse loss: 35.75592816610472, mae loss: 25.149567775816713, mape loss: 22.459758715748222

2024-06-04 22:02:17.861 | INFO     | __main__:main:138 - =====Epoch 5=====

##on train data## total loss: 0.034538953784712646, pred loss: 0.0278742902790659, regression loss: 0.006664663505646745 
##on train data## rmse loss: 37.80478400404348, mae loss: 25.766512615875627, mape loss: 26.49113877717429
##on valid data## rmse loss: 37.484771050548105, mae loss: 26.428007736025265, mape loss: 25.448025314587554
##on test data## rmse loss: 36.1664913141332, mae loss: 25.730495498078696, mape loss: 24.566423271504625

2024-06-04 22:06:52.228 | INFO     | __main__:main:138 - =====Epoch 6=====

##on train data## total loss: 0.033789841982436446, pred loss: 0.027239874793134583, regression loss: 0.00654996718930186 
##on train data## rmse loss: 36.5317303009753, mae loss: 24.056505323206103, mape loss: 19.928816790288348
##on valid data## rmse loss: 36.09297097011765, mae loss: 24.81309738882345, mape loss: 19.125780777484884
##on test data## rmse loss: 34.766239333491754, mae loss: 24.122228520741395, mape loss: 18.63650224785104

2024-06-04 22:11:27.193 | INFO     | __main__:main:138 - =====Epoch 7=====

##on train data## total loss: 0.03388397047389884, pred loss: 0.02727224954562086, regression loss: 0.006611720928277985 
##on train data## rmse loss: 36.60036049548935, mae loss: 24.69916355984766, mape loss: 25.042365436330908
##on valid data## rmse loss: 36.32999162086379, mae loss: 25.436716481972645, mape loss: 24.02689845005483
##on test data## rmse loss: 35.05638579960683, mae loss: 24.74019426644131, mape loss: 23.182198723063085

2024-06-04 22:16:01.300 | INFO     | __main__:main:138 - =====Epoch 8=====

##on train data## total loss: 0.033176116267535485, pred loss: 0.026669712921709946, regression loss: 0.0065064033458255375 
##on train data## rmse loss: 36.22821459980131, mae loss: 23.909555750073128, mape loss: 18.69533658496239
##on valid data## rmse loss: 36.07267335466864, mae loss: 24.90092598431483, mape loss: 18.04861610322767
##on test data## rmse loss: 34.76845240479962, mae loss: 24.21584589899434, mape loss: 17.607528112510934

2024-06-04 22:20:50.549 | INFO     | __main__:main:138 - =====Epoch 9=====

##on train data## total loss: 0.032772650201248484, pred loss: 0.026301896883066417, regression loss: 0.006470753318182069 
##on train data## rmse loss: 36.52835484570677, mae loss: 24.100009864231325, mape loss: 19.661040988358312
##on valid data## rmse loss: 35.857047316022395, mae loss: 24.744190103069865, mape loss: 18.76687297676977
##on test data## rmse loss: 34.81712672608724, mae loss: 24.231956983629562, mape loss: 18.4064259284763

2024-06-04 22:25:33.998 | INFO     | __main__:main:138 - =====Epoch 10=====

##on train data## total loss: 0.032438718707962604, pred loss: 0.02604325043378726, regression loss: 0.006395468274175348 
##on train data## rmse loss: 35.99207496343169, mae loss: 24.059606941990882, mape loss: 22.164191905432528
##on valid data## rmse loss: 35.923333882155575, mae loss: 24.943149697724113, mape loss: 21.4208170736288
##on test data## rmse loss: 34.647445931818815, mae loss: 24.275088432275854, mape loss: 20.6947595940382

2024-06-04 22:30:07.724 | INFO     | __main__:main:138 - =====Epoch 11=====

##on train data## total loss: 0.032003133270725515, pred loss: 0.0256327422395782, regression loss: 0.006370391031147314 
##on train data## rmse loss: 37.29689676656663, mae loss: 25.32707333114912, mape loss: 26.194799089300556
##on valid data## rmse loss: 36.91543812774369, mae loss: 25.93991722540833, mape loss: 25.07705731555749
##on test data## rmse loss: 35.789108393881556, mae loss: 25.366759200796697, mape loss: 24.225503065009818

2024-06-04 22:34:41.314 | INFO     | __main__:main:138 - =====Epoch 12=====

##on train data## total loss: 0.03210365733295773, pred loss: 0.02567665955841061, regression loss: 0.0064269977745471185 
##on train data## rmse loss: 35.288053257660295, mae loss: 23.24079973592698, mape loss: 18.867162884500043
##on valid data## rmse loss: 35.03834260696483, mae loss: 24.11402968094812, mape loss: 18.154213019598153
##on test data## rmse loss: 33.91319470608969, mae loss: 23.549357506901167, mape loss: 17.709706769594085

2024-06-04 22:39:28.529 | INFO     | __main__:main:138 - =====Epoch 13=====

##on train data## total loss: 0.031798798116451164, pred loss: 0.02541411786637439, regression loss: 0.006384680250076776 
##on train data## rmse loss: 35.533863349530684, mae loss: 23.589992751115524, mape loss: 21.0558932790029
##on valid data## rmse loss: 35.49559822805685, mae loss: 24.610952282403883, mape loss: 20.49053969541432
##on test data## rmse loss: 34.274903333582586, mae loss: 23.96274412977752, mape loss: 19.75928237794135

2024-06-04 22:44:19.344 | INFO     | __main__:main:138 - =====Epoch 14=====

##on train data## total loss: 0.03130416300897516, pred loss: 0.02502595931298902, regression loss: 0.006278203695986139 
##on train data## rmse loss: 35.21482407072055, mae loss: 23.320868483129537, mape loss: 19.814883486467338
##on valid data## rmse loss: 34.93173127377768, mae loss: 24.09870319908829, mape loss: 19.031566949958485
##on test data## rmse loss: 33.97295209694813, mae loss: 23.649620720560517, mape loss: 18.574149009740747

2024-06-04 22:49:10.193 | INFO     | __main__:main:138 - =====Epoch 15=====

##on train data## total loss: 0.031034809223862377, pred loss: 0.024800786156201467, regression loss: 0.006234023067660911 
##on train data## rmse loss: 35.38132762009243, mae loss: 23.438217200573135, mape loss: 18.15369004067385
##on valid data## rmse loss: 35.37627748968477, mae loss: 24.51155486717043, mape loss: 17.66185909862767
##on test data## rmse loss: 34.24347844056044, mae loss: 23.961254958292884, mape loss: 17.18710586075534

2024-06-04 22:54:01.216 | INFO     | __main__:main:138 - =====Epoch 16=====

##on train data## total loss: 0.030956877626876975, pred loss: 0.02474049425979713, regression loss: 0.006216383367079843 
##on train data## rmse loss: 34.73183404274707, mae loss: 22.884553636394955, mape loss: 19.0847497433424
##on valid data## rmse loss: 34.62985361921844, mae loss: 23.828280530269677, mape loss: 18.46281881510364
##on test data## rmse loss: 33.52435723978196, mae loss: 23.28800003449499, mape loss: 17.95529928323217

2024-06-04 22:58:51.598 | INFO     | __main__:main:138 - =====Epoch 17=====

##on train data## total loss: 0.030791232172024886, pred loss: 0.024600490932455345, regression loss: 0.006190741239569543 
##on train data## rmse loss: 34.72339938421669, mae loss: 22.862212615942806, mape loss: 17.883633073630197
##on valid data## rmse loss: 34.52893956351619, mae loss: 23.77986629088343, mape loss: 17.318978082935956
##on test data## rmse loss: 33.63362934691081, mae loss: 23.358506125861435, mape loss: 16.910615916500724

2024-06-04 23:03:27.249 | INFO     | __main__:main:138 - =====Epoch 18=====

##on train data## total loss: 0.030627416783604044, pred loss: 0.024480725094930815, regression loss: 0.006146691688673231 
##on train data## rmse loss: 34.95976293611827, mae loss: 23.41479450201838, mape loss: 22.589845789022416
##on valid data## rmse loss: 35.025163505879625, mae loss: 24.444693226384892, mape loss: 21.896677393625133
##on test data## rmse loss: 34.04679404163812, mae loss: 23.92182369141782, mape loss: 21.138033433265594

2024-06-04 23:07:59.907 | INFO     | __main__:main:138 - =====Epoch 19=====

##on train data## total loss: 0.030487079413478856, pred loss: 0.024391592326120082, regression loss: 0.006095487087358773 
##on train data## rmse loss: 34.689486950448476, mae loss: 22.7372446390068, mape loss: 17.386120413693618
##on valid data## rmse loss: 34.59282064663855, mae loss: 23.677638040334692, mape loss: 16.891451077579887
##on test data## rmse loss: 33.52200622468198, mae loss: 23.17776387336695, mape loss: 16.51336042590051

